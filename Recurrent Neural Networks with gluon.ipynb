{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:20.979639Z",
     "start_time": "2017-11-05T15:51:06.616584Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:20.990050Z",
     "start_time": "2017-11-05T15:51:20.981144Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:21.096625Z",
     "start_time": "2017-11-05T15:51:20.992555Z"
    }
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return mx.nd.array(ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:21.277105Z",
     "start_time": "2017-11-05T15:51:21.098631Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer = mx.init.Uniform(0.1))\n",
    "            if mode == 'rnn_relu':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            elif mode == 'lstm':\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden,\n",
    "                                        params = self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:21.370854Z",
     "start_time": "2017-11-05T15:51:21.279611Z"
    }
   },
   "outputs": [],
   "source": [
    "args_data = 'data/ptb/'\n",
    "args_model = 'rnn_tanh'\n",
    "args_emsize = 100\n",
    "args_nhid = 100\n",
    "args_nlayers = 2\n",
    "args_lr = 1.0\n",
    "args_clip = 0.2\n",
    "args_epochs = 100\n",
    "args_batch_size = 32\n",
    "args_bptt = 5\n",
    "args_dropout = 0.2\n",
    "args_tied = True\n",
    "args_cuda = 'store_true'\n",
    "args_log_interval = 500\n",
    "args_save = 'model.param'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:29.408856Z",
     "start_time": "2017-11-05T15:51:26.183372Z"
    }
   },
   "outputs": [],
   "source": [
    "context = mx.gpu(0)\n",
    "corpus = Corpus(args_data)\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(corpus.train, args_batch_size).as_in_context(context)\n",
    "val_data = batchify(corpus.valid, args_batch_size).as_in_context(context)\n",
    "test_data = batchify(corpus.test, args_batch_size).as_in_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:29.470351Z",
     "start_time": "2017-11-05T15:51:29.410220Z"
    }
   },
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = RNNModel(args_model, ntokens, args_emsize, args_nhid,\n",
    "                       args_nlayers, args_dropout, args_tied)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:30.849969Z",
     "start_time": "2017-11-05T15:51:30.839440Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:31.304677Z",
     "start_time": "2017-11-05T15:51:31.295152Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx=context)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T15:51:32.327808Z",
     "start_time": "2017-11-05T15:51:32.263545Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_val = float(\"Inf\")\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size, ctx = context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and bptt size to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            trainer.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = eval(val_data)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = eval(test_data)\n",
    "            model.save_params(args_save)\n",
    "            print('test loss %.2f, test perplexity %.2f' % (test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            args_lr = args_lr * 0.25\n",
    "            trainer._init_optimizer('sgd',\n",
    "                                    {'learning_rate': args_lr,\n",
    "                                     'momentum': 0,\n",
    "                                     'wd': 0})\n",
    "            model.load_params(args_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:26:38.809961Z",
     "start_time": "2017-11-05T15:52:14.944929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 500] loss 7.21, perplexity 1352.10\n",
      "[Epoch 1 Batch 1000] loss 6.43, perplexity 619.67\n",
      "[Epoch 1 Batch 1500] loss 6.25, perplexity 518.21\n",
      "[Epoch 1 Batch 2000] loss 6.17, perplexity 476.81\n",
      "[Epoch 1 Batch 2500] loss 6.05, perplexity 424.08\n",
      "[Epoch 1 Batch 3000] loss 5.94, perplexity 379.51\n",
      "[Epoch 1 Batch 3500] loss 5.94, perplexity 380.80\n",
      "[Epoch 1 Batch 4000] loss 5.82, perplexity 337.99\n",
      "[Epoch 1 Batch 4500] loss 5.81, perplexity 332.08\n",
      "[Epoch 1 Batch 5000] loss 5.80, perplexity 330.40\n",
      "[Epoch 1 Batch 5500] loss 5.80, perplexity 330.55\n",
      "[Epoch 1] time cost 219.19s, validation loss 5.67, validation perplexity 289.32\n",
      "test loss 5.64, test perplexity 280.64\n",
      "[Epoch 2 Batch 500] loss 5.78, perplexity 322.50\n",
      "[Epoch 2 Batch 1000] loss 5.71, perplexity 303.06\n",
      "[Epoch 2 Batch 1500] loss 5.68, perplexity 292.29\n",
      "[Epoch 2 Batch 2000] loss 5.71, perplexity 301.56\n",
      "[Epoch 2 Batch 2500] loss 5.66, perplexity 287.76\n",
      "[Epoch 2 Batch 3000] loss 5.58, perplexity 266.31\n",
      "[Epoch 2 Batch 3500] loss 5.63, perplexity 279.64\n",
      "[Epoch 2 Batch 4000] loss 5.54, perplexity 255.06\n",
      "[Epoch 2 Batch 4500] loss 5.53, perplexity 252.93\n",
      "[Epoch 2 Batch 5000] loss 5.56, perplexity 260.39\n",
      "[Epoch 2 Batch 5500] loss 5.58, perplexity 266.13\n",
      "[Epoch 2] time cost 224.33s, validation loss 5.48, validation perplexity 240.81\n",
      "test loss 5.45, test perplexity 232.43\n",
      "[Epoch 3 Batch 500] loss 5.59, perplexity 267.14\n",
      "[Epoch 3 Batch 1000] loss 5.54, perplexity 254.37\n",
      "[Epoch 3 Batch 1500] loss 5.52, perplexity 248.48\n",
      "[Epoch 3 Batch 2000] loss 5.56, perplexity 260.38\n",
      "[Epoch 3 Batch 2500] loss 5.53, perplexity 251.02\n",
      "[Epoch 3 Batch 3000] loss 5.45, perplexity 233.91\n",
      "[Epoch 3 Batch 3500] loss 5.51, perplexity 246.71\n",
      "[Epoch 3 Batch 4000] loss 5.42, perplexity 226.54\n",
      "[Epoch 3 Batch 4500] loss 5.41, perplexity 224.17\n",
      "[Epoch 3 Batch 5000] loss 5.45, perplexity 232.79\n",
      "[Epoch 3 Batch 5500] loss 5.48, perplexity 240.13\n",
      "[Epoch 3] time cost 221.74s, validation loss 5.39, validation perplexity 218.36\n",
      "test loss 5.35, test perplexity 209.71\n",
      "[Epoch 4 Batch 500] loss 5.50, perplexity 243.60\n",
      "[Epoch 4 Batch 1000] loss 5.45, perplexity 232.13\n",
      "[Epoch 4 Batch 1500] loss 5.43, perplexity 227.93\n",
      "[Epoch 4 Batch 2000] loss 5.49, perplexity 241.75\n",
      "[Epoch 4 Batch 2500] loss 5.45, perplexity 232.31\n",
      "[Epoch 4 Batch 3000] loss 5.38, perplexity 217.24\n",
      "[Epoch 4 Batch 3500] loss 5.43, perplexity 228.39\n",
      "[Epoch 4 Batch 4000] loss 5.35, perplexity 211.60\n",
      "[Epoch 4 Batch 4500] loss 5.35, perplexity 210.43\n",
      "[Epoch 4 Batch 5000] loss 5.39, perplexity 218.71\n",
      "[Epoch 4 Batch 5500] loss 5.42, perplexity 226.18\n",
      "[Epoch 4] time cost 215.58s, validation loss 5.34, validation perplexity 208.39\n",
      "test loss 5.30, test perplexity 199.88\n",
      "[Epoch 5 Batch 500] loss 5.44, perplexity 229.94\n",
      "[Epoch 5 Batch 1000] loss 5.40, perplexity 220.73\n",
      "[Epoch 5 Batch 1500] loss 5.38, perplexity 216.53\n",
      "[Epoch 5 Batch 2000] loss 5.43, perplexity 228.13\n",
      "[Epoch 5 Batch 2500] loss 5.40, perplexity 221.50\n",
      "[Epoch 5 Batch 3000] loss 5.33, perplexity 206.91\n",
      "[Epoch 5 Batch 3500] loss 5.39, perplexity 218.89\n",
      "[Epoch 5 Batch 4000] loss 5.31, perplexity 202.68\n",
      "[Epoch 5 Batch 4500] loss 5.30, perplexity 199.97\n",
      "[Epoch 5 Batch 5000] loss 5.34, perplexity 208.89\n",
      "[Epoch 5 Batch 5500] loss 5.38, perplexity 216.61\n",
      "[Epoch 5] time cost 214.73s, validation loss 5.31, validation perplexity 202.64\n",
      "test loss 5.26, test perplexity 193.33\n",
      "[Epoch 6 Batch 500] loss 5.40, perplexity 220.81\n",
      "[Epoch 6 Batch 1000] loss 5.36, perplexity 212.51\n",
      "[Epoch 6 Batch 1500] loss 5.33, perplexity 206.82\n",
      "[Epoch 6 Batch 2000] loss 5.39, perplexity 219.86\n",
      "[Epoch 6 Batch 2500] loss 5.37, perplexity 214.67\n",
      "[Epoch 6 Batch 3000] loss 5.30, perplexity 200.92\n",
      "[Epoch 6 Batch 3500] loss 5.36, perplexity 211.98\n",
      "[Epoch 6 Batch 4000] loss 5.27, perplexity 195.33\n",
      "[Epoch 6 Batch 4500] loss 5.27, perplexity 193.72\n",
      "[Epoch 6 Batch 5000] loss 5.31, perplexity 203.10\n",
      "[Epoch 6 Batch 5500] loss 5.35, perplexity 209.94\n",
      "[Epoch 6] time cost 213.18s, validation loss 5.29, validation perplexity 199.04\n",
      "test loss 5.24, test perplexity 189.19\n",
      "[Epoch 7 Batch 500] loss 5.37, perplexity 214.82\n",
      "[Epoch 7 Batch 1000] loss 5.32, perplexity 204.55\n",
      "[Epoch 7 Batch 1500] loss 5.31, perplexity 201.37\n",
      "[Epoch 7 Batch 2000] loss 5.37, perplexity 214.60\n",
      "[Epoch 7 Batch 2500] loss 5.34, perplexity 208.45\n",
      "[Epoch 7 Batch 3000] loss 5.27, perplexity 194.77\n",
      "[Epoch 7 Batch 3500] loss 5.33, perplexity 205.82\n",
      "[Epoch 7 Batch 4000] loss 5.25, perplexity 190.01\n",
      "[Epoch 7 Batch 4500] loss 5.24, perplexity 188.40\n",
      "[Epoch 7 Batch 5000] loss 5.29, perplexity 198.58\n",
      "[Epoch 7 Batch 5500] loss 5.32, perplexity 205.11\n",
      "[Epoch 7] time cost 211.61s, validation loss 5.27, validation perplexity 193.97\n",
      "test loss 5.22, test perplexity 185.07\n",
      "[Epoch 8 Batch 500] loss 5.34, perplexity 208.26\n",
      "[Epoch 8 Batch 1000] loss 5.30, perplexity 199.54\n",
      "[Epoch 8 Batch 1500] loss 5.28, perplexity 196.85\n",
      "[Epoch 8 Batch 2000] loss 5.34, perplexity 208.69\n",
      "[Epoch 8 Batch 2500] loss 5.32, perplexity 204.10\n",
      "[Epoch 8 Batch 3000] loss 5.25, perplexity 191.35\n",
      "[Epoch 8 Batch 3500] loss 5.30, perplexity 201.01\n",
      "[Epoch 8 Batch 4000] loss 5.23, perplexity 186.21\n",
      "[Epoch 8 Batch 4500] loss 5.21, perplexity 183.81\n",
      "[Epoch 8 Batch 5000] loss 5.27, perplexity 194.12\n",
      "[Epoch 8 Batch 5500] loss 5.31, perplexity 201.93\n",
      "[Epoch 8] time cost 212.07s, validation loss 5.25, validation perplexity 191.29\n",
      "test loss 5.21, test perplexity 182.22\n",
      "[Epoch 9 Batch 500] loss 5.32, perplexity 204.05\n",
      "[Epoch 9 Batch 1000] loss 5.28, perplexity 196.32\n",
      "[Epoch 9 Batch 1500] loss 5.26, perplexity 192.60\n",
      "[Epoch 9 Batch 2000] loss 5.33, perplexity 206.22\n",
      "[Epoch 9 Batch 2500] loss 5.29, perplexity 199.32\n",
      "[Epoch 9 Batch 3000] loss 5.23, perplexity 187.36\n",
      "[Epoch 9 Batch 3500] loss 5.29, perplexity 197.85\n",
      "[Epoch 9 Batch 4000] loss 5.22, perplexity 184.45\n",
      "[Epoch 9 Batch 4500] loss 5.20, perplexity 180.58\n",
      "[Epoch 9 Batch 5000] loss 5.26, perplexity 191.59\n",
      "[Epoch 9 Batch 5500] loss 5.29, perplexity 197.82\n",
      "[Epoch 9] time cost 216.11s, validation loss 5.25, validation perplexity 191.48\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'args_lr' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-afbb149ccfa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_L\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best test loss %.2f, test perplexity %.2f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-4600b0ee2bda>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test loss %.2f, test perplexity %.2f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_L\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0margs_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs_lr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             trainer._init_optimizer('sgd',\n\u001b[0;32m     42\u001b[0m                                     {'learning_rate': args_lr,\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'args_lr' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train()\n",
    "model.load_params(args_save, context)\n",
    "test_L = eval(test_data)\n",
    "print('Best test loss %.2f, test perplexity %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn_relu:\n",
    "Best test loss 5.19, test perplexity 178.71\n",
    "\n",
    "LSTM:\n",
    "Best test loss 4.81, test perplexity 122.95\n",
    "\n",
    "GRU:\n",
    "Best test loss 4.87, test perplexity 129.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
